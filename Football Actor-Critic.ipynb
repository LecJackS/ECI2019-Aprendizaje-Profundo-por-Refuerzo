{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Football Actor-Critic\n",
    "\n",
    "Partiendo de la base de un trabajo final con el algoritmo ***Asynchronous Advantage Actor Critic (A3C)*** para la [materia *Procesos Markovianos para el Aprendizaje Automático*](http://www.ic.fcen.uba.ar/actividades-academicas/formacion/cursos/procesos-markovianos-para-aprendizaje-automatico) dictada en Exactas, decidí adaptarlo para el entorno *football* a modo de comprobar qué tan bien generaliza.\n",
    "\n",
    "A3C **reemplaza el uso de memoria** de experiencias de modelos como DQN, **con varios procesos asincrónicos** devolviendo gradientes de sus respectivas redes locales, copias del modelo global.\n",
    "\n",
    "De esta forma se intenta acercar al modelo de Aprendizaje Supervisado, no muestreando sobre batchs de datos históricos, sino sobre secuencias de datos no tan correlacionadas entre sí.\n",
    "\n",
    "Modelo anterior para entrada de pixeles:\n",
    "![actor-critic-model-lstm](https://raw.githubusercontent.com/LecJackS/TP-Final-Procesos-Markovianos-para-el-Aprendizaje-Automatico-2019-1C-w.o.heavy-history-/master/img/actor-critic-model-lstm.png)\n",
    "\n",
    "La diferencia ahora es que la entrada serán features numéricas directamente, por lo que pueden descartarse las capas convolucionales y pasar directamente como features a las capas lineales, o para problemas/entornos con mayores relaciones temporales, una capa LSTM.\n",
    "\n",
    "Para el entorno de *football* se agregaron dos capas lineales de 64 unidades a modo de relacionar features diferentes entre sí (principalmente las de posiciones cardinales (x,y)), conectada a una capa lineal en lugar de la LSTM.\n",
    "\n",
    "![actor-critic-football-model.png](./img/actor-critic-football-model.png)\n",
    "\n",
    "El algoritmo elegido es Asynchronous Advantage Actor Critic (A3C), ya que brinda la posibilidad de aprovechar más eficientemente el uso del CPU, sin usar GPU.\n",
    "\n",
    "1. Los multiprocesos (**LOCALES**) obtienen una **copia** de los parámetros del modelo neuronal **GLOBAL** al iniciar la partida.\n",
    "\n",
    "2. Operan con esa copia hasta terminar la partida o definición de episodio.\n",
    "\n",
    "3. Calculan *Loss* similarmente a Q-Learning/SarsaMax.\n",
    "\n",
    "4. Actualizan los parámetros **GLOBALES** directamente con optimizador Adam.\n",
    "\n",
    "5. Repite desde 1., con varios procesos funcionando en paralelo.\n",
    "\n",
    "Esto sucede todo entre CPU y memoria, por lo que las actualizaciones y cálculos de gradientes se realizan a gran velocidad.\n",
    "\n",
    "![a3c](./img/a3c.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Índice de archivos\n",
    "\n",
    "En la notebook se encuentra la definición del agente y sus parámetros globales, pero fue necesario dividir parte del código en archivos individuales, por requerimientos de las librerías de multiprocesos de pytorch.\n",
    "\n",
    "1. ***local_train.py:*** Definición de un **proceso asincrónico particular**. Será usado por *train()* definida abajo en la notebook, para generar los actores.\n",
    "\n",
    "\n",
    "2. ***local_test.py:*** Similar a local_train, solo que **hace uso pasivo de los parámetros** de un nuevo modelo asincrónico (no actualiza/aprende ni calcula gradientes).\n",
    "\n",
    "\n",
    "3. ***env.py:*** Contiene *create_train_env()* que se encarga de **generar un entorno para football** (o cualquier otro env)\n",
    "\n",
    "\n",
    "4. ***model.py:*** **Modelo neuronal** con una capa lineal que recibe las features del entorno, conectada a otras dos capas individuales que reprensentan el Actor y el Crítico.\n",
    " \n",
    "  El ***Actor*** tendrá **una salida por cada acción** posible, que representa el **puntaje** de la misma, que se usará para calcular ***policy softmax***.\n",
    "  \n",
    "  El ***Crítico*** tendrá **una sola salida**: El valor de la **value function** para ese estado\n",
    "\n",
    "\n",
    "5. ***optimizer.py:*** Se definen los mismos optimizadores Adam y RMSProp de Pytorch, pero con **parámetros compartidos entre procesos asincrónicos y parámetros globales**.\n",
    "\n",
    "\n",
    "Para más detalles del funcionamiento del algoritmo A3C, dejo la notebook del trabajo realizado anteriormente mencionado, con más definiciones teóricas e intuición detrás del mismo:\n",
    "\n",
    "[Very quick roadmap to Asynchronous Advantage Actor Critic.ipynb](https://github.com/LecJackS/TP-Final-Procesos-Markovianos-para-el-Aprendizaje-Automatico-2019-1C-w.o.heavy-history-/blob/master/Very%20quick%20roadmap%20to%20Asynchronous%20Advantage%20Actor%20Critic.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sobre gfootball\n",
    "\n",
    "Todos los valores del estado (para *simple115*) son valores entre -1 y 1, por lo que se decidió dejarlos de esa forma.\n",
    "\n",
    "Para una entrada de píxeles los valores de cada uno están entre 0 y 255, que suelen normalizarse según la media y varianza de la imagen completa (o un batch), por lo que se esperan ciertas diferencias en cuanto al aprendizaje inicial del agente.\n",
    "\n",
    "Los rewards son también de -1 y 1.\n",
    "\n",
    "En *env.py* se definió un *wrapper* sobre el cual se puede controlar con más detalle a los rewards, agregando también descuentos por cada timestep.\n",
    "\n",
    "Los resultados de varias pruebas indicaron que penalizar por tiempo retraza (al menos al comienzo) el aprendizaje del agente.\n",
    "\n",
    "La causa de ésto posiblemente esté relacionada con lo que menciona Vlad Mnih en el siguiente video:\n",
    "\n",
    "***Reinforcement Learning 9: A Brief Tour of Deep RL Agents*** @1:31:50 https://youtu.be/-mhBD8Frkc4?t=5515\n",
    "\n",
    "Donde explica que **rewards simples** del tipo \"buena o mala\" acción, **facilitan el aprendizaje** simplificando el problema a **obtener una mayor cantidad de acciones positivas**.\n",
    "\n",
    "Agregando penalización por tiempo los rewards serán valores muy variables, y por más que estén en un rango fijo (ej -1, 1) el agente tiene dificultad para aprender de ellos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimentación\n",
    "\n",
    "A pesar de verse mejoras en el tiempo con rewards simples sin penalidad, cada cierto intervalo se notó un decaimiento en performance, hasta alcanzar valores muy bajos sin recuperarse.\n",
    "\n",
    "A continuación, plots de:\n",
    "\n",
    "1. **Reward acumulado** por cada episodio\n",
    "2. **Reward promedio** de los pasados **100 episodios**\n",
    "3. **Mediana** del los últimos **100 episodios**\n",
    "![big-lr.png](./img/big-lr.png)\n",
    "\n",
    "Notar los picos en plots de reward promedio, poco después de los 500 episodios, 1300 y 1600.\n",
    "\n",
    "Luego de algunas pruebas con resultados similares, era indicio de un learning rate muy alto, resultando en divergencia por pasos demasiado grandes.\n",
    "\n",
    "Se decidió por reiniciar el aprendizaje desde cero, pero a los 500 episodios, **se redujo el learning rate** en un órden de magnitud (de 1e-4 a 1e-5).\n",
    "\n",
    "![smaller-lr.png](./img/smaller-lr.png)\n",
    "\n",
    "Este cambio produjo resultados muy positivos en la estabilidad del algoritmo en el tiempo, permitiendo mantenerse entre el 60-80% de juegos ganados, con tendencia a seguir aumentando en el tiempo.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TO-DO: \n",
    "\n",
    "Otros modelos más complejos (con LSTM) fueron probados **con todos los mapas** de manera aleatoria en cada episodio, observando muy pocas mejoras.\n",
    "\n",
    "Experimentos limitados al mapa simple permitieron ver que un modelo con una capa de LSTM **no** obtiene los resultados que se llegan a obtener con una simple feedforward layer.\n",
    "\n",
    "Ésto indica que falta regular hiperparámetros o simplemente necesita mucho más tiempo para comenzar a mostrar resultados.\n",
    "\n",
    "Se intentó explorar ambas posibilidades durante estas semanas, corriendo tres experimentos a la vez en tres computadoras, pero no fue suficiente.\n",
    "\n",
    "Faltaría más análisis en el problema para encontrar posibles causas y ser eficiente con el tiempo invertido."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Código\n",
    "\n",
    "Las siguientes 3 celdas de código permiten entrenar un agente para un modelo lineal desde cero.\n",
    "\n",
    "Para elegir otro modelo basta modificar AC_NN_MODEL en *local_train.py* (y en la celda de Interfaz abajo) con los modelos neuronales disponibles en *model.py*\n",
    "\n",
    "Las últimas 2 celdas ejecutan un agente en modo *Test* ya entrenado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global parameters: Agent definition\n",
    "\n",
    "Se define el modelo de parámetros globales, y los procesos asincrónicos que actuarán como *actors* en el proceso de aprendizaje, devolviendo gradientes de experiencias, en forma de updates paralelos asincrónicos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-15T12:50:52.482935Z",
     "start_time": "2019-08-15T12:50:52.471116Z"
    }
   },
   "outputs": [],
   "source": [
    "def train(opt):\n",
    "    torch.manual_seed(42)\n",
    "    # Prepare log directory\n",
    "    if os.path.isdir(opt.log_path):\n",
    "        shutil.rmtree(opt.log_path)\n",
    "    os.makedirs(opt.log_path)\n",
    "    # Prepare saved models directory\n",
    "    if not os.path.isdir(opt.saved_path):\n",
    "        os.makedirs(opt.saved_path)\n",
    "    # Prepare multiprocessing\n",
    "    mp = _mp.get_context(\"spawn\")\n",
    "    # Create new training environment just to get number\n",
    "    # of inputs and outputs to neural network\n",
    "    _, num_states, num_actions = create_train_env(opt.layout, opt.num_processes_to_render)\n",
    "    # Create Neural Network model\n",
    "    global_model = AC_NN_MODEL(num_states, num_actions)\n",
    "    if opt.use_gpu:\n",
    "        global_model.cuda()\n",
    "    # Share memory with processes for optimization later on\n",
    "    global_model.share_memory()\n",
    "    # Load trained agent weights\n",
    "    if opt.load_previous_weights:\n",
    "        file_ = \"{}/gfootball_{}\".format(opt.saved_path, opt.layout)\n",
    "        if os.path.isfile(file_):\n",
    "            print(\"Loading previous weights for %s...\" %opt.layout, end=\" \")\n",
    "            # global_model.load_state_dict(torch.load(file_))\n",
    "            pretrained_dict = torch.load(file_)\n",
    "            global_model_dict = global_model.state_dict()\n",
    "            # 1. filter out unnecessary keys (if trained with different model)\n",
    "            pretrained_dict = {k: v for k, v in pretrained_dict.items() if k in global_model_dict}\n",
    "            \n",
    "            # 2. overwrite entries in the existing state dict\n",
    "            global_model_dict.update(pretrained_dict) \n",
    "            # 3. load the new state dict (only existing keys)\n",
    "            global_model.load_state_dict(pretrained_dict, strict=False)\n",
    "            print(\"Done.\")\n",
    "        else:\n",
    "            print(\"Can't load any previous weights for %s! Starting from scratch...\" %opt.layout)\n",
    "    # Define optimizer with shared weights. See 'optimizer.py'\n",
    "    optimizer = GlobalRMSprop(global_model.parameters(), lr=opt.lr)\n",
    "    # Create async processes\n",
    "    processes = []\n",
    "    for index in range(opt.num_processes):\n",
    "        # Multiprocessing async agents\n",
    "        if index == 0:\n",
    "            # Save weights to file only with this process\n",
    "            process = mp.Process(target=local_train, args=(index, opt, global_model, optimizer, True))\n",
    "            \n",
    "        else:\n",
    "            process = mp.Process(target=local_train, args=(index, opt, global_model, optimizer))\n",
    "        process.start()\n",
    "        processes.append(process)\n",
    "        \n",
    "    # Local test simulation (creates another model = more memory used)\n",
    "    #process = mp.Process(target=local_test, args=(opt.num_processes, opt, global_model))\n",
    "    #process.start()\n",
    "    #processes.append(process)\n",
    "    for process in processes:\n",
    "        process.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-15T12:50:55.018620Z",
     "start_time": "2019-08-15T12:50:52.485628Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "# To NOT use OpenMP threads within numpy processes\n",
    "os.environ['OMP_NUM_THREADS'] = '1'\n",
    "import argparse\n",
    "import torch\n",
    "from model import SimpleActorCriticLineal\n",
    "AC_NN_MODEL = SimpleActorCriticLineal\n",
    "# For the async policies updates\n",
    "import torch.multiprocessing as _mp\n",
    "# _multiprocessing needs functions to be imported from their own file\n",
    "# so it does not work if functions are defined on this jupyter notebook\n",
    "from local_train import local_train #\n",
    "from env import create_train_env\n",
    "from optimizer import GlobalAdam, GlobalRMSprop\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Agent\n",
    "\n",
    "Basta correr la siguiente celda (luego de las anteriores) para simular una cantidad ***num_processes*** de procesos asincrónicos, pudiendo visualizarlas con el control  ***num_processes_to_render***.\n",
    "\n",
    "En ./tensorboard se guardará una carpeta con estadísticas del agente.\n",
    "\n",
    "Para visualizarla:\n",
    "\n",
    "    tensorboard --logdir=some_id:./tensorboard/\n",
    "    # or\n",
    "    tensorboard --logdir=some_id:/home/USER/ECI2019-Aprendizaje-Profundo-por-Refuerzo/tensorboard/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-15T12:55:00.169773Z",
     "start_time": "2019-08-15T12:53:52.106694Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Los detalles del aprendizaje se muestran en la TERMINAL (not here)\n",
      "\n",
      "Getting number of NN inputs/outputs for desde_cero\n",
      "Can't load any previous weights for desde_cero! Starting from scratch...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-f9ab96d98ad0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m                  log_path='tensorboard')\n\u001b[1;32m     17\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Los detalles del aprendizaje se muestran en la TERMINAL (not here)\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-1-0f3d9313ec48>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(opt)\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;31m#processes.append(process)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mprocess\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprocesses\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m         \u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.7/multiprocessing/process.py\u001b[0m in \u001b[0;36mjoin\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_pid\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetpid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'can only join a child process'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_popen\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'can only join a started process'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_popen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mres\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m             \u001b[0m_children\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiscard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.7/multiprocessing/popen_fork.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m     46\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0;31m# This shouldn't block if wait() returned successfully.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWNOHANG\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0.0\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.7/multiprocessing/popen_fork.py\u001b[0m in \u001b[0;36mpoll\u001b[0;34m(self, flag)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                 \u001b[0mpid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwaitpid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m                 \u001b[0;31m# Child process not yet created. See #1731717\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from argparse import Namespace\n",
    "args = Namespace(beta =0.01,\n",
    "                 gamma=0.99,\n",
    "                 tau=1.0,\n",
    "                 lr=1e-4,\n",
    "                 layout='desde_cero',\n",
    "                 load_previous_weights=True,\n",
    "                 num_processes=8,                # numero de procesos asincronicos\n",
    "                 num_processes_to_render=1,     # numero de procesos a visualizar\n",
    "                 use_gpu=False,\n",
    "                 max_actions=200,\n",
    "                 num_global_steps=5e9,\n",
    "                 num_local_steps=50,     # async updates every\n",
    "                 save_interval=10,\n",
    "                 saved_path='trained_models',\n",
    "                 log_path='tensorboard')\n",
    "print(\"Los detalles del aprendizaje se muestran en la TERMINAL (not here)\\n\")\n",
    "train(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Agent\n",
    "\n",
    "A continuación un agente entrenado con un modelo lineal que acierta la mayoría de las veces en el mapa de entrenamiento:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-15T12:52:54.391650Z",
     "start_time": "2019-08-15T12:52:54.382608Z"
    }
   },
   "outputs": [],
   "source": [
    "def test(opt):\n",
    "    torch.manual_seed(42)\n",
    "    # Prepare saved models directory\n",
    "    if not os.path.isdir(opt.saved_path):\n",
    "        os.makedirs(opt.saved_path)\n",
    "    # Prepare multiprocessing\n",
    "    mp = _mp.get_context(\"spawn\")\n",
    "    # Create new training environment just to get number\n",
    "    # of inputs and outputs to neural network\n",
    "    _, num_states, num_actions = create_train_env(opt.layout, 1)\n",
    "    # Create Neural Network model\n",
    "    global_model = AC_NN_MODEL(num_states, num_actions)\n",
    "    # Share memory with processes for optimization later on\n",
    "    global_model.share_memory()\n",
    "    # Load trained agent weights\n",
    "    file_ = \"{}/gfootball-lineal-75\".format(opt.saved_path)\n",
    "    if os.path.isfile(file_):\n",
    "        print(\"Loading previous weights for %s...\" %opt.layout, end=\" \")\n",
    "        # global_model.load_state_dict(torch.load(file_))\n",
    "        pretrained_dict = torch.load(file_)\n",
    "        global_model_dict = global_model.state_dict()\n",
    "        # 1. filter out unnecessary keys (if trained with different model)\n",
    "        pretrained_dict = {k: v for k, v in pretrained_dict.items() if k in global_model_dict}\n",
    "\n",
    "        # 2. overwrite entries in the existing state dict\n",
    "        global_model_dict.update(pretrained_dict) \n",
    "        # 3. load the new state dict (only existing keys)\n",
    "        global_model.load_state_dict(pretrained_dict, strict=False)\n",
    "        print(\"Done.\")\n",
    "    else:\n",
    "        print(\"Can't load any previous weights for %s! Starting from scratch...\" %opt.layout)\n",
    " \n",
    "    #Local test simulation\n",
    "    process = mp.Process(target=local_test, args=(1, opt, global_model))\n",
    "    process.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Para ejecutar varios agentes al mismo tiempo (en modo test), ejecutar la siguiente celda varias veces:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-15T12:52:55.310286Z",
     "start_time": "2019-08-15T12:52:55.267448Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Los detalles de los episodios se muestran en la TERMINAL (not here)\n",
      "\n",
      "Getting number of NN inputs/outputs for football\n",
      "Loading previous weights for football... Done.\n"
     ]
    }
   ],
   "source": [
    "from local_test import local_test\n",
    "from argparse import Namespace\n",
    "args = Namespace(layout='football',\n",
    "                 load_previous_weights=True,\n",
    "                 max_actions=200,\n",
    "                 num_global_steps=5e9,\n",
    "                 saved_path='trained_models')\n",
    "print(\"Los detalles de los episodios se muestran en la TERMINAL (not here)\\n\")\n",
    "\n",
    "test(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
